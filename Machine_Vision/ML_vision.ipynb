{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4a1f9c",
   "metadata": {},
   "source": [
    "# Use the User Interface to start the program after running the .ipynb\n",
    "\n",
    "# Description: \n",
    "\n",
    "This program provides several easily customizable Computer Vision features for images and videos.\n",
    "Results will be saved to the \"OUTPUT\" folder.\n",
    "Multiple features can be simultaneously applied onto a single image or video, or the features can be applied separately and individually. \n",
    "Video input can be either livestreamed or a previously saved video.\n",
    "\n",
    "1. Face Detection\n",
    "\n",
    "Uses pre-trained ML models to identify faces and classify the emotions and gender of people in an image/video. \n",
    "The faces on the image will be marked with rectangular bounding boxes, and the emotions and gender of the faces will be displayed above each box. The total amount of faces detected will be displayed in the top left corner. \n",
    "Be aware that this is currently the most computationally expensive of the features. Improvements will be made in the future.\n",
    "\n",
    "How to use:\n",
    "Ensure that the \"Face detection\" button is set to True depending on whether you want to use images or videos.\n",
    "Drag images to the \"INPUT\" folder or videos to the \"INPUT_VIDEO\" folder.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Template Matching\n",
    "\n",
    "Locates a smaller \"template image\" inside a larger image/video using your choice from a variety of OpenCV methods. The best matches will be marked with bounding boxes.\n",
    "\n",
    "How to use:\n",
    "Ensure that the \"Template matching\" button is set to True depending on whether you want to use images or videos.\n",
    "Customize constant parameters to choose which OpenCV methods to use.\n",
    "Drag a single chosen template image to the \"TEMPLATE\" folder to compare it against each of the images in the \"INPUT\" folder or for each of the videos in the \"INPUT_VIDEO\" folder. Template must be smaller than compared images/frames or an Exception will be raised. If there are multiple template images in the \"TEMPLATE\" folder, only the first will be selected according to alphabetical order.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Optical Flow\n",
    "\n",
    "Draws the optical flow detected in a sequence of images or the frames of a video. \n",
    "\n",
    "How to use:\n",
    "Ensure that the \"Optical flow\" button is set to True depending on whether you want to use images or videos.\n",
    "Drag two or more images to the \"INPUT\" folder or videos to the \"INPUT_VIDEO\" folder.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4. SIFT (Scale Invariant Feature Transform) \n",
    "\n",
    "Compares the SIFT keypoints between two images or between one images and a video. \n",
    "Lines will be drawn to show which keypoints match. \n",
    "\n",
    "How to use:\n",
    "Ensure that the \"SIFT matching\" button is set to True depending on whether you want to use images or videos.\n",
    "Drag the image to be compared to the \"SIFT\" folder to compare it against each of the images in the \"INPUT\" folder or for each of the videos in the \"INPUT_VIDEO\" folder it. If there are multiple images in the \"SIFT\" folder, only the first will be selected according to alphabetical order.\n",
    "\n",
    "\n",
    "# Other considerations\n",
    "\n",
    "The \"Customizable program constants\" cell is where all constants that can be modified are found. This is where you can select which features to use or customize the parameters of the features. All constants are well documented with comments.\n",
    "\n",
    "Multiple images or videos can be processed in a list. \n",
    "All images must be .jpg, .jpeg, or .png (but alpha channel not preserved)\n",
    "All images must be .mp4\n",
    "\n",
    " \n",
    "Example input images will be found in the \"data\" folder.\n",
    "\n",
    "The \"utils\" folder contains the defined functions used in the program.\n",
    "\n",
    "The \"trained_models\" folder contains the pretrained face detection models sourced from OpenCV. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66e3e5",
   "metadata": {},
   "source": [
    "# Credits:\n",
    "Project programmer:\n",
    "Ethan Eckmann\n",
    "\n",
    "Project mentor: \n",
    "Xandeep Alexander\n",
    "\n",
    "Tutorials and code provided by OpenCV and others were crucial in creating this project.\n",
    "\n",
    "https://github.com/opencv\n",
    "Face detection, emotion classification, and gender classification machine learning models sourced from OpenCV. \n",
    "\n",
    "https://github.com/oarriaga/face_classification\n",
    "This is where some of the Face Detection code originated. Many modifications were made over the course of the project.\n",
    "\n",
    "https://docs.opencv.org/4.x/d4/dc6/tutorial_py_template_matching.html\n",
    "Template Matching tutorial.\n",
    "\n",
    "https://learnopencv.com/optical-flow-in-opencv/\n",
    "Optical Flow tutorial.\n",
    "\n",
    "https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html\n",
    "SIFT tutorial.\n",
    "\n",
    "https://github.com/pinecone-io/examples/blob/master/learn/search/image/image-retrieval-ebook/bag-of-visual-words/bag-of-visual-words.ipynb\n",
    "SIFT bag of visual words / bag of features tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ace775",
   "metadata": {},
   "source": [
    "# Known issues:\n",
    "\n",
    "1.\n",
    "\n",
    "Text and bounding boxes may be difficult to see depending on your image/video dimensions. You must customize text and bounding box parameters manually.\n",
    "\n",
    "2.\n",
    "\n",
    "Face detection does not work well with a series of images with varying dimsensions and varying distances of faces from the camera. The face detection parameters must be customized for the composition and dimensions of your chosen images. \n",
    "\n",
    "3.\n",
    "\n",
    "The program will produce consistent face detection results for an image, but may produce a different set of consistent results for the same image that has been flipped horizontally (or otherwise edited). Flipping done in the Windows Photo app, any other photo editor, or using cv2.flip() function can change the results. \n",
    "\n",
    "Initially having the photo flipped at all in photo editors causes different number of faces to be detected and emotion/gender classification to be different.\n",
    "\n",
    "Initially having the photo flipped an odd number of times using cv2 functions causes emotion/gender classification to be different only.\n",
    "\n",
    "Initially having the photo flipped an even number of times using cv2 functions changes nothing.\n",
    "\n",
    "Differences in emotion, and gender accuracy due to orientation could be caused by the data each respective machine learning model was trained on.\n",
    "\n",
    "However, the face detection for each image will always be processed in both its orignal state, AND its horizontally flipped state (using cv2) for the most consistent face detection results. The face detection results from both versions of the image are essentially \"merged\" together before emotion and gender classification. Despite this, the inconsistency problem with face detection still persists. \n",
    "\n",
    "I can only speculate that this problem is not caused by any fault of this program but rather caused by metadata changes or differences in .jpg compression techniques between cv2 and various photo editors.\n",
    "\n",
    "4.\n",
    "\n",
    "The emotion classifier struggles to identify disgust.\n",
    "\n",
    "5.\n",
    "\n",
    "Face Detection with livestream video suffers from stuttering on each position update frame. This is due to the high computational cost of the face detection feature. Other features have stuttering as well, but it is most noticable here. I will work on fixing this flaw in the future. \n",
    "\n",
    "6.\n",
    "\n",
    "Template matching often has low accuracy unless the template is found one-to-one inside an image. This is an inherent weakness of the simplistic template matching algorithim, and template matching was included in this program primarily to showcase the much more robust capabilities of SIFT matching.\n",
    "\n",
    "7.\n",
    "\n",
    "Optical flow only tracks the best points initially detected on the first frame of a video. In the future, there will be an option to change this to instead have an update rate.  \n",
    "\n",
    "8.\n",
    "\n",
    "SIFT detection struggles when there is heavy noise such as camera blur.\n",
    "\n",
    "9.\n",
    "\n",
    "Any form of SIFT detection with videos can sometimes create videos of very large resolution. This may cause the videos to be incompatable with windows media player. To get around this, you can use VLC media player instead.\n",
    "\n",
    "10.\n",
    "\n",
    "Optical flow with images will raise an exception if the list of images provided are of varying dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a9e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Features and improvements that will be implemented in the future:\n",
    "    # Organize main functions\n",
    "    # More customization for optical flow, including clear screen intervals \n",
    "    # Reduce frame stuttering for livestream\n",
    "    # Advanced Bag of features using SIFT\n",
    "\n",
    "# low priority:\n",
    "    # Better documentation for functions \n",
    "    # Change Optical Flow image to work with list of image so that colors are consistent \n",
    "    # Make face detection more computationally efficient\n",
    "    # Make super functions more efficient by removing .copy()s without causing unintentional modifications \n",
    "    # Eigenfaces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de4a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "from utils.datasets import get_labels\n",
    "from utils.detectfaces import face_detection, face_detection_video, face_detection_stream\n",
    "from utils.templatematch import template_match, template_match_video, template_match_stream\n",
    "from utils.opticalflow import optical_flow, optical_flow_video, optical_flow_stream\n",
    "from utils.sift import sift_compare, sift_compare_video, sift_compare_stream\n",
    "from utils.super import super_image, super_video, super_stream\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Output, VBox, Button, ToggleButton\n",
    "from IPython.display import display\n",
    "import time\n",
    "from jupyter_ui_poll import ui_events\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7616a1",
   "metadata": {},
   "source": [
    "# User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f709b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI output\n",
    "output_area = Output()\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1b306",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Customizable program constants\n",
    "\n",
    "    # -- I/O PARAMETERS --\n",
    "# String.\n",
    "# Folder for images to be input for face detection/analysis, template matching, optical flow, and SIFT comparison.\n",
    "INPUT_IMAGE_FOLDER = 'INPUT'\n",
    "\n",
    "# String.\n",
    "# Folder for a single video to be input for face detection/analysis, template matching, optical flow and SIFT comparison.\n",
    "INPUT_VIDEO_FOLDER = 'INPUT_VIDEO'\n",
    "\n",
    "# String.\n",
    "# Folder for a single template image to be input for the purpose of template matching against images or videos.\n",
    "INPUT_TEMPLATE_FOLDER = 'TEMPLATE'\n",
    "\n",
    "# String.\n",
    "# Folder for a single image to be input for the purpose of SIFT comparison against images or videos.\n",
    "INPUT_SIFT_FOLDER = 'SIFT'\n",
    "\n",
    "# String.\n",
    "# Folder for images or videos to be output.\n",
    "OUTPUT_FOLDER = 'OUTPUT'\n",
    "\n",
    "\n",
    "\n",
    "    # -- ENABLE/DISABLE PROGRAM FEATURES --\n",
    "# Boolean.\n",
    "# If True, performs all enabled features on each image to generate super images, rather than using each of the enabled features separately. \n",
    "# Images will be saved to OUTPUT_FOLDER.\n",
    "# Observe the 'value' property of the toggle button\n",
    "SUPER_IMAGE = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs all enabled features on each video to generate a super video, rather than using each of the enabled features separately. \n",
    "# Videos will be saved to OUTPUT_FOLDER.\n",
    "SUPER_VIDEO = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs face detection and analysis for each image in the INPUT_IMAGE_FOLDER. Draws bounding boxes, identifies the emotion and gender of each face, and counts the number of faces.\n",
    "# Images will be saved to OUTPUT_FOLDER.\n",
    "FACEDETECT_IMG_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs template match for each image in INPUT_IMAGE_FOLDER using a single template image in INPUT_TEMPLATE_FOLDER using the cv2 template matching method specified by TEMPLATEMATCH_IMG_METHOD. \n",
    "# Images will be saved to OUTPUT_FOLDER.\n",
    "TEMPLATEMATCH_IMG_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs optical flow using a sequence of images in INPUT_IMAGE_FOLDER. Requires more than one image in INPUT_IMAGE_FOLDER.\n",
    "# Images will be saved to OUTPUT_FOLDER.\n",
    "OPFLOW_IMG_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs SIFT (scale invariant feature transform) using a single image in INPUT_SIFT_FOLDER against all images in INPUT_IMAGE_FOLDER.\n",
    "# Note: The saved images will have higher dimensions then the original INPUT_FOLDER images because the SIFT image is added onto the side of each INPUT_FOLDER image, expanding the width and/or height.\n",
    "# Images will be saved to OUTPUT_FOLDER.\n",
    "SIFT_IMG_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs face detection and analysis for each video in INPUT_VIDEO_FOLDER. Draws bounding boxes, identifies the emotion and gender of each face, and counts the number of faces.\n",
    "# Videos will be saved to OUTPUT_FOLDER.\n",
    "FACEDETECT_VID_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs template match for each video in INPUT_VIDEO_FOLDER using the single template in INPUT_TEMPLATE_FOLDER using the cv2 template matching method specified by TEMPLATEMATCH_VID_METHOD.\n",
    "# Videos will be saved to OUTPUT_FOLDER.\n",
    "TEMPLATEMATCH_VID_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs optical flow for each video in INPUT_VIDEO_FOLDER.\n",
    "# Videos will be saved to OUTPUT_FOLDER.\n",
    "OPFLOW_VID_BOOL = False\n",
    "\n",
    "# Boolean.\n",
    "# If True, performs SIFT (scale invariant feature transform) using a single image in INPUT_SIFT_FOLDER against all videos in INPUT_VIDEO_FOLDER.\n",
    "# Note: The saved video will have higher dimensions then your camera resolution because the SIFT image is added onto the side of the video, expanding the width and/or height\n",
    "# Videos will be saved to OUTPUT_FOLDER.\n",
    "SIFT_VID_BOOL = False\n",
    "\n",
    "\n",
    "# Integer.\n",
    "# How many frames in the video must pass before face detection, template matching, and SIFT data is updated again.\n",
    "# Higher values lead to worse accuracy but faster processing time. \n",
    "# Data is always initially detected on the first frame of the video.\n",
    "# EX: VID_UPDATE_RATE = 10, means that bounding boxes updated every 10 frames.\n",
    "VID_UPDATE_RATE = 20\n",
    "\n",
    "# Boolean.\n",
    "# If True, use the computer's camera to livestream video feed instead of videos in the INPUT_VIDEO_FOLDER.\n",
    "# The livestream will appear as a resizeable window \n",
    "# Optionally save video.\n",
    "# The camera is specified using LIVESTREAM_INPUT.\n",
    "# The camera's settings will be used for determining frame size and frame rate.\n",
    "# Note: OpenCV is not DPI aware, so windows DPI scaling may cause the window appear too large or too small by default.\n",
    "# Livestream can be closed by clicking the red X button on the window or by pressing the specified LIVESTREAM_ENDKEY.\n",
    "LIVESTREAM_BOOL = False\n",
    "\n",
    "# Integer or URL String.\n",
    "# 0 for default webcam, or provide another camera index.\n",
    "# Cameras connected to your system are typically assigned numerical indices starting from 0. Your default webcam is usually index 0. Other connected cameras will have subsequent indices (e.g., 1, 2, etc.).\n",
    "# If you are trying to access an IP camera or network stream, you would pass the URL of the stream to cv2.VideoCapture() instead of a numerical index.\n",
    "LIVESTREAM_INPUT = 0\n",
    "\n",
    "# String.\n",
    "# If this key on the keyboard is pressed, end the livestream.\n",
    "LIVESTREAM_ENDKEY = 'q'\n",
    "\n",
    "# Boolean.\n",
    "# If True, saves the livestream to OUTPUT_FOLDER after closing.\n",
    "LIVESTREAM_SAVE_BOOL = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # -- COLOR PARAMETERS -- \n",
    "# Tuple of integers.\n",
    "# Example colors in the Blue, Green, Red format used by CV2.\n",
    "BLUE = (255, 0, 0) \n",
    "GREEN = (0, 255, 0) \n",
    "RED = (0, 0, 255) \n",
    "CYAN = (255, 255, 0)\n",
    "MAGENTA = (255, 0, 255)\n",
    "YELLOW = (0, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "WHITE = (255, 255, 255)\n",
    "\n",
    "# Color of displayed emotion and gender text.\n",
    "TEXT_COLOR = RED\n",
    "\n",
    "# Color of the total faces detected text placed in the corner of the image when using face detection.\n",
    "TEXT_COLOR_TF = RED\n",
    "\n",
    "# Color of all text outlines.\n",
    "OUTLINE_TEXT_COLOR = BLACK\n",
    "\n",
    "# Color of face detection bounding boxes.\n",
    "BOX_COLOR = RED\n",
    "\n",
    "# Color of the template match bounding boxes.\n",
    "TEMPLATEMATCH_BOX_COLOR = MAGENTA\n",
    "\n",
    "# Color of the template match method text.\n",
    "TEMPLATEMATCH_TEXT_COLOR = MAGENTA\n",
    "\n",
    "# Colors that coordinate with the cv2 template matching methods:\n",
    "# methods = ['TM_SQDIFF', 'TM_SQDIFF_NORMED', 'TM_CCORR', 'TM_CCORR_NORMED', 'TM_CCOEFF', 'TM_CCOEFF_NORMED']\n",
    "# Only used if:\n",
    "# 1. TEMPLATEMATCH_VID_METHODCHOICE is not specified and TEMPLATEMATCH_VID_BOOL = True\n",
    "# 2. TEMPLATEMATCH_IMG_METHODCHOICE is not specified and SUPER_IMAGE = True\n",
    "TM_METHODS_COLOR = [BLUE, GREEN, RED, CYAN, MAGENTA, YELLOW]\n",
    "\n",
    "# Color for lines of optical flow\n",
    "# If not specified, a list of 100 random colors will be used \n",
    "OPFLOW_COLOR = None\n",
    "\n",
    "# Color for lines that connect SIFT matched keypoints and the color of the matched keypoints.\n",
    "# If None, colors will be random.\n",
    "SIFT_LINE_COLOR = GREEN\n",
    "\n",
    "# Color for points that mark the location of non-matched SIFT keypoints.\n",
    "# If None, colors will be random.\n",
    "SIFT_POINT_COLOR = MAGENTA\n",
    "\n",
    "\n",
    "\n",
    "    # -- TEXT AND BOUNDING BOX PARAMETERS --     \n",
    "# OUTLINE_TEXT_BOOL - Boolean. If True, all text will be outlined.\n",
    "# TEXT_SIZE - Float. Font scale of text. When scale > 1, the text is magnified. When 0 < scale < 1, the text is minimized. When scale < 0, the text is mirrored or reversed.\n",
    "# TEXT_THICK - Integer. Thickness of text.\n",
    "# BOX_THICK - Integer. Bounding box thickness for face detection and template matching.\n",
    "\n",
    "# Good starting variables for large sized images:\n",
    "OUTLINE_TEXT_BOOL = True\n",
    "TEXT_SIZE = 2\n",
    "TEXT_THICK = 2\n",
    "BOX_THICK = 3\n",
    "\n",
    "# Good starting variables for medium sized images:\n",
    "# OUTLINE_TEXT_BOOL = True\n",
    "# TEXT_SIZE = 1\n",
    "# TEXT_THICK = 2\n",
    "# BOX_THICK = 2\n",
    "\n",
    "# Good starting variables for small sized images:\n",
    "# OUTLINE_TEXT_BOOL = True\n",
    "# TEXT_SIZE = 0.4\n",
    "# TEXT_THICK = 1\n",
    "# BOX_THICK = 1\n",
    "\n",
    "\n",
    "\n",
    "    # -- FACE DETECTION AND CLASSIFICATION PARAMETERS --\n",
    "# Face detection parameters:\n",
    "# scaleFactor - Float. How much the image size is reduced at each image scale. Higher value results in slower processing time but more thorough search\n",
    "# minNeighbors - Integer. How many neighbors each candidate rectangle should have to retain it. Higher value results in fewer detections\n",
    "# minSize - Tuple of integers. Minimum possible object size in pixels\n",
    "# maxSize - Tuple of integers. Maximum possible object size in pixels\n",
    "\n",
    "    # Good starting variables for medium sized images.\n",
    "# parameters for face detection (facing forwards):\n",
    "SCALEFACTOR_F = 1.01 \n",
    "MINNEIGHBORS_F = 40\n",
    "MINSIZE_F = (125, 125)\n",
    "MAXSIZE_F = None\n",
    "# parameters for face detection (side profile):\n",
    "SCALEFACTOR_S = 1.01\n",
    "MINNEIGHBORS_S = 40\n",
    "MINSIZE_S = (125, 125)\n",
    "MAXSIZE_S = None\n",
    "\n",
    "    # Good starting variables for small sized images.\n",
    "# parameters for face detection (facing forwards):\n",
    "# SCALEFACTOR_F = 1.1 \n",
    "# MINNEIGHBORS_F = 10\n",
    "# MINSIZE_F = (10, 10)\n",
    "# MAXSIZE_F = None\n",
    "# # parameters for face detection (side profile):\n",
    "# SCALEFACTOR_S = 1.1\n",
    "# MINNEIGHBORS_S = 10\n",
    "# MINSIZE_S = (10, 10)\n",
    "# MAXSIZE_S = None\n",
    "\n",
    "\n",
    "    # alternative, good starting variables for small sized images.\n",
    "# parameters for face detection (facing forwards):\n",
    "# SCALEFACTOR_F = 1.05\n",
    "# MINNEIGHBORS_F = 20\n",
    "# MINSIZE_F = (10, 10)\n",
    "# MAXSIZE_F = (200,200)\n",
    "# # parameters for face detection (side profile):\n",
    "# SCALEFACTOR_S = 1.05\n",
    "# MINNEIGHBORS_S = 20\n",
    "# MINSIZE_S = (10, 10)\n",
    "# MAXSIZE_S = (200,200)\n",
    "\n",
    "# Tuple of integers.\n",
    "# The area for emotion and gender classification expands outwards from the bounding box of a face by an offset.\n",
    "# These constants do NOT change the bounding box size on the image.\n",
    "# Recommended to to keep at EMOTION_OFFSETS = (20, 40), GENDER_OFFSETS = (30, 60).\n",
    "EMOTION_OFFSETS = (20, 40)\n",
    "GENDER_OFFSETS = (30, 60)\n",
    "\n",
    "\n",
    "    # -- TEMPLATE MATCHING PARAMETERS --\n",
    "# List of Strings.\n",
    "# If any of these conditions are true, all methods in TEMPLATEMATCH_METHODS will be used:\n",
    "# 1. TEMPLATEMATCH_VID_METHODCHOICE is not specified and TEMPLATEMATCH_VID_BOOL = True\n",
    "# 2. TEMPLATEMATCH_IMG_METHODCHOICE is not specified and SUPER_IMAGE = True\n",
    "# The possible CV2 methods for template matching:\n",
    "# methods = ['TM_SQDIFF', 'TM_SQDIFF_NORMED', 'TM_CCORR', 'TM_CCORR_NORMED', 'TM_CCOEFF', 'TM_CCOEFF_NORMED']\n",
    "TEMPLATEMATCH_METHODS = ['TM_SQDIFF', 'TM_SQDIFF_NORMED', 'TM_CCORR', 'TM_CCORR_NORMED', 'TM_CCOEFF', 'TM_CCOEFF_NORMED']\n",
    "\n",
    "# Integer.\n",
    "# The amount of template matching results to be displayed on the image using a bounding box.\n",
    "# After the best template match result is found and a bounding box is drawn, the same template image sized area can't be selected twice for the next template match result. This means that overlap usually only occurs when TEMPLATEMATCH_AMOUNT is a very large number.\n",
    "# Only used for image template match, not video.\n",
    "# Cannot be lower than 1.\n",
    "TEMPLATEMATCH_AMOUNT = 1\n",
    "\n",
    "# String.\n",
    "# Choice of method for template matching in images.\n",
    "# If TEMPLATEMATCH_IMG_METHODCHOICE is not found in TEMPLATEMATCH_METHODS: all methods in TEMPLATEMATCH_METHODS will be considered for each image in INPUT. If SUPER_IMAGE = True, all methods will be simultaneously applied onto each of the input images. Otherwise, each method will be applied individually for each input image, which can lead to a large amount of resulting images to be created.  \n",
    "TEMPLATEMATCH_IMG_METHODCHOICE = 'TM_SQDIFF_NORMED'\n",
    "\n",
    "# String.\n",
    "# Choice of method for template matching in videos.\n",
    "# If TEMPLATEMATCH_VID_METHODCHOICE is not found in TEMPLATEMATCH_METHODS: all methods in TEMPLATEMATCH_METHODS will be simultaneously applied for each video.\n",
    "TEMPLATEMATCH_VID_METHODCHOICE = None\n",
    "\n",
    "\n",
    "\n",
    "    # -- OPTICAL FLOW PARAMETERS --\n",
    "# Integer.\n",
    "# How many random colors should be generated in a list if optical flow features are enabled but OPFLOW_COLOR is not specified. Note that colors can be repeated.\n",
    "# Each random color will correlate with an optical flow line and point. \n",
    "# If the list of random colors is smaller than the number of detected optical flow points then the program will raise an indexOutOfBounds error, so it is important that this number is at least 1000 to be safe. \n",
    "# If not specified, defaults to 1000.\n",
    "OPFLOW_TOTAL_RANDOM_COLORS = 1000\n",
    "\n",
    "\n",
    "\n",
    "    # -- SIFT PARAMETERS --\n",
    "# Float.\n",
    "# Used to discard ambiguous SIFT matches.\n",
    "# Higher values lead to more SIFT matches detected, lower values lead to less SIFT matches detected.\n",
    "# Explanation:\n",
    "# Each keypoint of the first image is matched with a number of keypoints from the second image. We keep the 2 best matches for each keypoint (best matches = the ones with the smallest distance measurement). Lowe's test checks that the two distances are sufficiently different. If they are not, then the keypoint is determined to be ambiguous and is eliminated. \n",
    "    # Simple example:\n",
    "    # Initally, the two best matches are \n",
    "    # m.distance = 309, n.distance = 329\n",
    "    # the ratio_threshold = 0.20 and when multiplied with n.distance = 65\n",
    "\n",
    "    # logic:\n",
    "        # if m.distance < ratio_threshold*n.distance:\n",
    "        #   m is a good match\n",
    "        # else:\n",
    "        #   m is a bad match\n",
    "\n",
    "    # appling the logic\n",
    "        # 309 < 65:\n",
    "        # result: m is a bad match\n",
    "        # lower ratio_threshold values causes less matches.\n",
    "SIFT_RATIO_THRESHOLD = 0.65\n",
    "\n",
    "# Enumeration (Integer).\n",
    "# Three options:\n",
    "# cv2.DrawMatchesFlags_DEFAULT (0) - Mark all SIFT features in the images using circles with color = SIFT_POINT_COLOR and draw lines with color = SIFT_LINE_COLOR to connect matches.\n",
    "# cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS (2) - Draw lines with color = SIFT_LINE_COLOR to connect matches only.\n",
    "# cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS (4) - Mark all SIFT features with keypoint size and orientation in the images using circles with color = SIFT_POINT_COLOR and draw lines with color = SIFT_LINE_COLOR to connect matches.\n",
    "# If you do not want any lines on the image, you must set SIFT_LINE_COLOR = None.\n",
    "# Combine multiple flags with with \" | \"\n",
    "# Ex: SIFT_FLAG = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS | cv2.DrawMatchesFlags_DRAW_RICH_KEYPOINTS\n",
    "# Note: The cv2.DrawMatchesFlags_DRAW_OVER_OUTIMG (1) flag is currently not implemented in this program due to the fact that it would be difficult to implement and would conflict with super videos or super images. Trying to use it will cause an exception to be raised. An example of how this flag would work normally is included in the data/examples folder for viewing.\n",
    "# Note:\n",
    "# It technically is possible to remove the lines, but because the lines include matched keypoints, removing the lines and also keeping MATCHED keypoints is impossible using the cv2.drawMatchesKnn function. Workarounds are possible, but one was not implemented for this program.\n",
    "SIFT_FLAG = cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
    "\n",
    "# Boolean.\n",
    "# If SIFT_SIDE is True, then the SIFT image will appear to the right of the image/video it is compared to \n",
    "# If SIFT_SIDE is False, then the SIFT image will appear to the left of the imag/video it is compared to \n",
    "SIFT_RSIDE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd385c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UI for important boolean values and starting program\n",
    "# Program waits until buttons pressed\n",
    "# Most of UI code AI generated for convenience \n",
    "\n",
    "# Dictionary to hold the ToggleButton widgets for easier management\n",
    "button_widgets = {}\n",
    "\n",
    "# Dictionary to map variable names (strings) to the actual global boolean variables\n",
    "# This helps in referencing them in the observer function\n",
    "global_boolean_vars = {\n",
    "    'Super Image': lambda: SUPER_IMAGE, \n",
    "    'Super Video': lambda: SUPER_VIDEO,\n",
    "    'Livestream': lambda: LIVESTREAM_BOOL,\n",
    "    'Save livestream': lambda: LIVESTREAM_SAVE_BOOL,\n",
    "    'Face detection for images': lambda: FACEDETECT_IMG_BOOL,\n",
    "    'Template matching for images': lambda: TEMPLATEMATCH_IMG_BOOL,\n",
    "    'Optical flow for images': lambda: OPFLOW_IMG_BOOL,\n",
    "    'SIFT matching for images': lambda: SIFT_IMG_BOOL,\n",
    "    'Face detection for videos': lambda: FACEDETECT_VID_BOOL,\n",
    "    'Template matching for videos': lambda: TEMPLATEMATCH_VID_BOOL,\n",
    "    'Optical flow for videos': lambda: OPFLOW_VID_BOOL,\n",
    "    'SIFT matching for videos': lambda: SIFT_VID_BOOL,\n",
    "}\n",
    "\n",
    "# Define the observer function (updated to handle global variables)\n",
    "def on_toggle_change(change, var_name):\n",
    "    global SUPER_IMAGE, SUPER_VIDEO, LIVESTREAM_BOOL, LIVESTREAM_SAVE_BOOL, FACEDETECT_IMG_BOOL, TEMPLATEMATCH_IMG_BOOL, OPFLOW_IMG_BOOL, SIFT_IMG_BOOL, FACEDETECT_VID_BOOL, TEMPLATEMATCH_VID_BOOL, OPFLOW_VID_BOOL, SIFT_VID_BOOL # Declare global to modify\n",
    "\n",
    "    # Update the corresponding global boolean variable based on var_name\n",
    "    if var_name == 'Super Image':\n",
    "        SUPER_IMAGE = change.new\n",
    "    elif var_name == 'Super Video':\n",
    "        SUPER_VIDEO = change.new\n",
    "    elif var_name == 'Livestream':\n",
    "        LIVESTREAM_BOOL = change.new\n",
    "    elif var_name == 'Save livestream':\n",
    "        LIVESTREAM_SAVE_BOOL = change.new\n",
    "    elif var_name == 'Face detection for images':\n",
    "        FACEDETECT_IMG_BOOL = change.new\n",
    "    elif var_name == 'Template matching for images':\n",
    "        TEMPLATEMATCH_IMG_BOOL = change.new\n",
    "    elif var_name == 'Optical flow for images':\n",
    "        OPFLOW_IMG_BOOL = change.new\n",
    "    elif var_name == 'SIFT matching for images':\n",
    "        SIFT_IMG_BOOL = change.new\n",
    "    elif var_name == 'Face detection for videos':\n",
    "        FACEDETECT_VID_BOOL = change.new\n",
    "    elif var_name == 'Template matching for videos':\n",
    "        TEMPLATEMATCH_VID_BOOL = change.new\n",
    "    elif var_name == 'Optical flow for videos':\n",
    "        OPFLOW_VID_BOOL = change.new\n",
    "    elif var_name == 'SIFT matching for videos':\n",
    "        SIFT_VID_BOOL = change.new\n",
    "\n",
    "    # Update the output area to show the change\n",
    "    with output_area:\n",
    "        # Retrieve the updated value from the global variable\n",
    "        updated_value = change.new # This directly reflects the button's new state  \n",
    "\n",
    "    # Update the button's icon and style based on its new state\n",
    "    button = button_widgets[var_name]\n",
    "    if change.new:\n",
    "        button.icon = 'check'\n",
    "        button.button_style = 'primary'\n",
    "    else:\n",
    "        button.icon = ''\n",
    "        button.button_style = 'danger'\n",
    "\n",
    "# Create buttons and attach observers\n",
    "for name, get_initial_value_func in global_boolean_vars.items():\n",
    "    initial_value = get_initial_value_func()\n",
    "    button_style = 'primary' if initial_value else 'danger'\n",
    "\n",
    "    button = ToggleButton(\n",
    "        value=initial_value,\n",
    "        description=name,\n",
    "        button_style=button_style,\n",
    "        tooltip=f'Toggle {name}',\n",
    "        icon='check' if initial_value else '',\n",
    "        layout=Layout(width='200px', height='50px')\n",
    "    )\n",
    "    button_widgets[name] = button\n",
    "\n",
    "    # Attach the observer, passing the variable name using a lambda for closure\n",
    "    button.observe(lambda change, var_name_for_closure=name: on_toggle_change(change, var_name_for_closure), names='value')\n",
    "\n",
    "# Arrange the buttons in a vertical box\n",
    "buttons_box = VBox(list(button_widgets.values()))\n",
    "\n",
    "with output_area: # Display the widgets\n",
    "    print(\"Interact with the buttons below:\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Boolean to start and stop program\n",
    "is_running = False\n",
    "# Button to start program\n",
    "def start_program(b):\n",
    "    # Callback function for the start button\n",
    "    global is_running\n",
    "    if not is_running:\n",
    "        is_running = True\n",
    "        # Start main long_running_task in a separate thread\n",
    "        # This is crucial so the Jupyter kernel doesn't freeze\n",
    "        # while the task is running.\n",
    "        # thread = threading.Thread(target=long_running_task(output_area))\n",
    "        # thread.start()\n",
    "        start_button.disabled = True\n",
    "        with output_area:\n",
    "            print(\"Starting program...\")\n",
    "\n",
    "# Create the buttons\n",
    "start_button = Button(description=\"Start Program\", button_style='success')\n",
    "\n",
    "# Register the callback functions\n",
    "start_button.on_click(start_program)\n",
    "\n",
    "with output_area:\n",
    "    display(buttons_box, start_button)\n",
    "\n",
    "with ui_events() as poll:\n",
    "    while not is_running:\n",
    "        poll(10)  # Poll for UI events, including button clicks\n",
    "        time.sleep(0.1)  # Briefly sleep to avoid busy-waiting\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc9b63-c32c-4d7e-85af-50ed4d467b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# paths for the pre-trained models \n",
    "detection_model_front_path = 'trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
    "detection_model_side_path = 'trained_models/detection_models/haarcascade_profileface.xml'\n",
    "emotion_model_path = 'trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
    "gender_model_path = 'trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5'\n",
    "\n",
    "# loading models\n",
    "# face detection and emotion/gender models need to be loaded differently\n",
    "FACEDETECTION_FRONT = cv2.CascadeClassifier(detection_model_front_path)\n",
    "FACEDETECTION_SIDE = cv2.CascadeClassifier(detection_model_side_path)\n",
    "EMOTION_CLASSIFIER = load_model(emotion_model_path, compile=False)\n",
    "GENDER_CLASSIFIER = load_model(gender_model_path, compile=False)\n",
    "\n",
    "# labels for the fer2013 dataset that emotion model is using\n",
    "# emotion_labels: {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
    "EMOTION_LABELS = get_labels('fer2013')\n",
    "\n",
    "# labels for the imdb dataset that gender model is using \n",
    "# gender_labels: {0: 'woman', 1: 'man'}\n",
    "GENDER_LABELS = get_labels('imdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503beabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading images\n",
    "\n",
    "image_list = []\n",
    "video_paths_list = []\n",
    "image_extensions = ('.jpg', '.jpeg', '.png') \n",
    "video_extensions = ('.mp4')\n",
    "# sorts numerically for numbers and alphabetically (Unicode comparison) for strings, in ascending order\n",
    "filenames_images = sorted([f for f in os.listdir(INPUT_IMAGE_FOLDER) if f.lower().endswith(image_extensions)]) \n",
    "filenames_video = sorted([f for f in os.listdir(INPUT_VIDEO_FOLDER) if f.lower().endswith(video_extensions)]) \n",
    "filenames_templates = sorted([f for f in os.listdir(INPUT_TEMPLATE_FOLDER) if f.lower().endswith(image_extensions)]) \n",
    "filenames_sift = sorted([f for f in os.listdir(INPUT_SIFT_FOLDER) if f.lower().endswith(image_extensions)]) \n",
    "\n",
    "# Error handling\n",
    "if (len(filenames_images) == 0 or filenames_images is None) and (FACEDETECT_IMG_BOOL or TEMPLATEMATCH_IMG_BOOL or OPFLOW_IMG_BOOL or SIFT_IMG_BOOL):\n",
    "    sys.exit(f'Error: No images were input. Place image(s) in the {INPUT_IMAGE_FOLDER} folder.') \n",
    "\n",
    "if LIVESTREAM_BOOL == False and (len(filenames_video) == 0 or filenames_video is None) and (FACEDETECT_VID_BOOL or TEMPLATEMATCH_VID_BOOL or OPFLOW_VID_BOOL or SIFT_VID_BOOL):\n",
    "    sys.exit(f'Error: No videos were input. Place video(s) in the {INPUT_VIDEO_FOLDER} folder.') \n",
    "\n",
    "if LIVESTREAM_BOOL and (FACEDETECT_VID_BOOL == False and TEMPLATEMATCH_VID_BOOL == False and OPFLOW_VID_BOOL == False and SIFT_VID_BOOL == False):\n",
    "    sys.exit(f'Error: Livestream enabled but no video features enabled.') \n",
    "\n",
    "if LIVESTREAM_BOOL:\n",
    "    c = cv2.VideoCapture(LIVESTREAM_INPUT)\n",
    "    if not c.isOpened():\n",
    "        print(f\"Error: Could not open camera with index {LIVESTREAM_INPUT}\")\n",
    "\n",
    "if (len(filenames_templates) == 0 or filenames_templates is None) and (TEMPLATEMATCH_IMG_BOOL or TEMPLATEMATCH_VID_BOOL):\n",
    "    sys.exit(f'Error: Template match for images is enabled, but no templates were provided. Place a template image in the {INPUT_TEMPLATE_FOLDER} folder.') \n",
    "\n",
    "if (len(filenames_images) < 2 or filenames_images is None) and OPFLOW_IMG_BOOL:\n",
    "    sys.exit(f'Error: Optical flow for images is not possible without two or more images. Place image(s) in the {INPUT_IMAGE_FOLDER} folder.') \n",
    "\n",
    "if (len(filenames_sift) == 0 or filenames_sift is None) and (SIFT_IMG_BOOL or SIFT_VID_BOOL):\n",
    "    sys.exit(f'Error: SIFT match for images is enabled, but the SIFT image to be compared was not provided. Place a image in the {INPUT_SIFT_FOLDER} folder.') \n",
    "\n",
    "\n",
    "image_list = []\n",
    "# if image is enabled, create list of read images to be used in program \n",
    "if FACEDETECT_IMG_BOOL or TEMPLATEMATCH_IMG_BOOL or OPFLOW_IMG_BOOL or SIFT_IMG_BOOL:\n",
    "    for f in filenames_images:\n",
    "        full_path = os.path.join(INPUT_IMAGE_FOLDER, f)\n",
    "        image = cv2.imread(full_path, cv2.IMREAD_COLOR)\n",
    "        image_list.append(image)\n",
    "\n",
    "video_paths_list = []\n",
    "# if video is enabled, create list of video paths\n",
    "if FACEDETECT_VID_BOOL or TEMPLATEMATCH_VID_BOOL or OPFLOW_VID_BOOL or SIFT_VID_BOOL:\n",
    "    for f in filenames_video:\n",
    "        full_path = os.path.join(INPUT_VIDEO_FOLDER, f)\n",
    "        video_paths_list.append(full_path)\n",
    "\n",
    "# if template matching is enabled, select only the first template image from TEMPLATE\n",
    "template = None\n",
    "if TEMPLATEMATCH_IMG_BOOL or TEMPLATEMATCH_VID_BOOL:\n",
    "    full_path = os.path.join(INPUT_TEMPLATE_FOLDER, filenames_templates[0])\n",
    "    template = cv2.imread(full_path, cv2.IMREAD_COLOR)\n",
    "\n",
    "# if SIFT is enabled, select only the first image from SIFT\n",
    "sift_image = None\n",
    "if SIFT_IMG_BOOL or SIFT_VID_BOOL:\n",
    "    full_path = os.path.join(INPUT_SIFT_FOLDER, filenames_sift[0])\n",
    "    sift_image = cv2.imread(full_path, cv2.IMREAD_COLOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5367e9-db02-4d57-a809-6a456cab44fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calls\n",
    "\n",
    "# Perform image features separately\n",
    "\n",
    "\n",
    "if not SUPER_IMAGE:\n",
    "    # Optional: Face detection, emotion and gender classification for each image in INPUT\n",
    "    if FACEDETECT_IMG_BOOL:\n",
    "        for i in range(len(image_list)):\n",
    "            filename = f'facedetect_{i+1}'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            path, img = face_detection(image_list[i], FACEDETECTION_FRONT, SCALEFACTOR_F, MINNEIGHBORS_F, MINSIZE_F, MAXSIZE_F, FACEDETECTION_SIDE, SCALEFACTOR_S, MINNEIGHBORS_S, MINSIZE_S, MAXSIZE_S, EMOTION_CLASSIFIER, GENDER_CLASSIFIER, EMOTION_OFFSETS, GENDER_OFFSETS, EMOTION_LABELS, GENDER_LABELS, BOX_COLOR, BOX_THICK, TEXT_COLOR, TEXT_SIZE, TEXT_THICK, TEXT_COLOR_TF, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path)\n",
    "            cv2.imwrite(path, img)\n",
    "\n",
    "    # Optional: Template match compare using the single template image in TEMPLATE against each image in INPUT \n",
    "    if TEMPLATEMATCH_IMG_BOOL:\n",
    "        paths_list = []\n",
    "        img_list = []\n",
    "        for i in range(len(image_list)):\n",
    "            filename = f'templatematch_{i+1}'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            temp_paths_list, temp_img_list = template_match(image_list[i], template, TEMPLATEMATCH_METHODS, TEMPLATEMATCH_BOX_COLOR, TEMPLATEMATCH_TEXT_COLOR, TEXT_SIZE, TEXT_THICK, BOX_THICK, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path, TEMPLATEMATCH_IMG_METHODCHOICE, TEMPLATEMATCH_AMOUNT)\n",
    "            paths_list = paths_list + temp_paths_list\n",
    "            img_list = img_list + temp_img_list\n",
    "        for j in range(len(paths_list)):\n",
    "            cv2.imwrite(paths_list[j], img_list[j])\n",
    "\n",
    "    # Optional: Optical flow for the sequence of images in INPUT\n",
    "    if OPFLOW_IMG_BOOL:\n",
    "        for i in range(len(image_list) -1):\n",
    "            filename = f'opflow_{i+1}-{i+2}'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            path, img = optical_flow(image_list[i], image_list[i+1], full_output_path, total_random_colors=OPFLOW_TOTAL_RANDOM_COLORS, color=OPFLOW_COLOR)\n",
    "            cv2.imwrite(path, img)\n",
    "\n",
    "    # Optional: Compare SIFT features of one image against all other images \n",
    "    if SIFT_IMG_BOOL:\n",
    "        for i in range(len(image_list)): # start index at 1 to skip the first element \n",
    "            filename = f'SIFT_{i+1}'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            path, img = sift_compare(image_list[i], sift_image, SIFT_RATIO_THRESHOLD, SIFT_LINE_COLOR, SIFT_POINT_COLOR, SIFT_RSIDE, full_output_path, SIFT_FLAG)\n",
    "            cv2.imwrite(path, img)\n",
    "\n",
    "\n",
    "# SUPER IMAGE\n",
    "# Perform all image features together on each image\n",
    "# The image markups from one feature will not have any effect on the results of another feature\n",
    "if SUPER_IMAGE:  \n",
    "    filename = 'super'\n",
    "    full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "    super_image(image_list, FACEDETECTION_FRONT, SCALEFACTOR_F, MINNEIGHBORS_F, MINSIZE_F, MAXSIZE_F, FACEDETECTION_SIDE, SCALEFACTOR_S, MINNEIGHBORS_S, MINSIZE_S, MAXSIZE_S, EMOTION_CLASSIFIER, GENDER_CLASSIFIER, EMOTION_OFFSETS, GENDER_OFFSETS, EMOTION_LABELS, GENDER_LABELS, template, TEMPLATEMATCH_METHODS, TM_METHODS_COLOR, TEMPLATEMATCH_IMG_METHODCHOICE, TEMPLATEMATCH_AMOUNT, sift_image, SIFT_RATIO_THRESHOLD, SIFT_LINE_COLOR, SIFT_POINT_COLOR, SIFT_RSIDE, SIFT_FLAG, BOX_COLOR, BOX_THICK, TEXT_COLOR, TEXT_SIZE, TEXT_THICK, TEXT_COLOR_TF, TEMPLATEMATCH_BOX_COLOR, TEMPLATEMATCH_TEXT_COLOR, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path, FACEDETECT_IMG_BOOL, TEMPLATEMATCH_IMG_BOOL, OPFLOW_IMG_BOOL, SIFT_IMG_BOOL,OPFLOW_TOTAL_RANDOM_COLORS, OPFLOW_COLOR)\n",
    "\n",
    "\n",
    "# Perform video features separately\n",
    "if not SUPER_VIDEO:\n",
    "    # Optional: Face detection, emotion and gender classification using each video in INPUT_VIDEO or with livestream\n",
    "    if FACEDETECT_VID_BOOL:\n",
    "        if LIVESTREAM_BOOL:\n",
    "            filename = f'facedetect_stream'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            face_detection_stream(LIVESTREAM_INPUT, LIVESTREAM_ENDKEY, LIVESTREAM_SAVE_BOOL, VID_UPDATE_RATE, FACEDETECTION_FRONT, SCALEFACTOR_F, MINNEIGHBORS_F, MINSIZE_F, MAXSIZE_F, FACEDETECTION_SIDE, SCALEFACTOR_S, MINNEIGHBORS_S, MINSIZE_S, MAXSIZE_S, EMOTION_CLASSIFIER, GENDER_CLASSIFIER, EMOTION_OFFSETS, GENDER_OFFSETS, EMOTION_LABELS, GENDER_LABELS, BOX_COLOR, BOX_THICK, TEXT_COLOR, TEXT_SIZE, TEXT_THICK, TEXT_COLOR_TF, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path)\n",
    "        else:\n",
    "            for i in range(len(video_paths_list)):\n",
    "                filename = f'facedetect_vid_{i+1}'\n",
    "                full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "                face_detection_video(video_paths_list[i], VID_UPDATE_RATE, FACEDETECTION_FRONT, SCALEFACTOR_F, MINNEIGHBORS_F, MINSIZE_F, MAXSIZE_F, FACEDETECTION_SIDE, SCALEFACTOR_S, MINNEIGHBORS_S, MINSIZE_S, MAXSIZE_S, EMOTION_CLASSIFIER, GENDER_CLASSIFIER, EMOTION_OFFSETS, GENDER_OFFSETS, EMOTION_LABELS, GENDER_LABELS, BOX_COLOR, BOX_THICK, TEXT_COLOR, TEXT_SIZE, TEXT_THICK, TEXT_COLOR_TF, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path)\n",
    "\n",
    "    # Optional: Template match compare using the single template image in TEMPLATE against each video in INPUT_VIDEO or with livestream\n",
    "    if TEMPLATEMATCH_VID_BOOL:\n",
    "        if LIVESTREAM_BOOL:\n",
    "            filename = f'templatematch_stream'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            template_match_stream(LIVESTREAM_INPUT, LIVESTREAM_ENDKEY, LIVESTREAM_SAVE_BOOL, VID_UPDATE_RATE, template, TEMPLATEMATCH_METHODS, TEMPLATEMATCH_BOX_COLOR, TEMPLATEMATCH_TEXT_COLOR, TEXT_SIZE, TEXT_THICK, BOX_THICK, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path, TM_METHODS_COLOR, TEMPLATEMATCH_VID_METHODCHOICE)\n",
    "        else:\n",
    "            for i in range(len(video_paths_list)):\n",
    "                filename = f'templatematch_vid_{i+1}'\n",
    "                full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "                template_match_video(video_paths_list[i], VID_UPDATE_RATE, template, TEMPLATEMATCH_METHODS, TEMPLATEMATCH_BOX_COLOR, TEMPLATEMATCH_TEXT_COLOR, TEXT_SIZE, TEXT_THICK, BOX_THICK, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path, TM_METHODS_COLOR, TEMPLATEMATCH_VID_METHODCHOICE)\n",
    "\n",
    "    # Optional: Optical flow using each video in INPUT_VIDEO\n",
    "    if OPFLOW_VID_BOOL:\n",
    "        if LIVESTREAM_BOOL:\n",
    "            filename = f'opflow_stream'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            optical_flow_stream(LIVESTREAM_INPUT, LIVESTREAM_ENDKEY, LIVESTREAM_SAVE_BOOL, full_output_path, OPFLOW_TOTAL_RANDOM_COLORS, OPFLOW_COLOR)\n",
    "        else:\n",
    "            for i in range(len(video_paths_list)):\n",
    "                filename = f'opflow_vid_{i+1}'\n",
    "                full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "                optical_flow_video(video_paths_list[i], full_output_path, OPFLOW_TOTAL_RANDOM_COLORS, OPFLOW_COLOR)\n",
    "\n",
    "    # Optional: Compare SIFT features of one image against all videos \n",
    "    if SIFT_VID_BOOL:\n",
    "        if LIVESTREAM_BOOL:\n",
    "            filename = f'SIFT_stream'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            sift_compare_stream(LIVESTREAM_INPUT, LIVESTREAM_ENDKEY, LIVESTREAM_SAVE_BOOL, VID_UPDATE_RATE, sift_image, SIFT_RATIO_THRESHOLD, SIFT_LINE_COLOR, SIFT_POINT_COLOR, SIFT_RSIDE, full_output_path, SIFT_FLAG)\n",
    "        else:\n",
    "            for i in range(len(video_paths_list)):\n",
    "                filename = f'SIFT_vid_{i+1}'\n",
    "                full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "                sift_compare_video(video_paths_list[i], VID_UPDATE_RATE, sift_image, SIFT_RATIO_THRESHOLD, SIFT_LINE_COLOR, SIFT_POINT_COLOR, SIFT_RSIDE, full_output_path, SIFT_FLAG)\n",
    "\n",
    "\n",
    "# SUPER VIDEO\n",
    "# Perform all video features together on each video\n",
    "# The frame markups from one feature will not have any effect on the results of another feature\n",
    "if SUPER_VIDEO:\n",
    "    if LIVESTREAM_BOOL:\n",
    "        filename = f'super_stream'\n",
    "        full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "        super_stream(LIVESTREAM_INPUT, LIVESTREAM_ENDKEY, LIVESTREAM_SAVE_BOOL, VID_UPDATE_RATE, FACEDETECTION_FRONT, SCALEFACTOR_F, MINNEIGHBORS_F, MINSIZE_F, MAXSIZE_F, FACEDETECTION_SIDE, SCALEFACTOR_S, MINNEIGHBORS_S, MINSIZE_S, MAXSIZE_S, EMOTION_CLASSIFIER, GENDER_CLASSIFIER, EMOTION_OFFSETS, GENDER_OFFSETS, EMOTION_LABELS, GENDER_LABELS, template, TEMPLATEMATCH_METHODS, TM_METHODS_COLOR, TEMPLATEMATCH_VID_METHODCHOICE, sift_image, SIFT_RATIO_THRESHOLD, SIFT_LINE_COLOR, SIFT_POINT_COLOR, SIFT_RSIDE, SIFT_FLAG, BOX_COLOR, BOX_THICK, TEXT_COLOR, TEXT_SIZE, TEXT_THICK, TEXT_COLOR_TF, TEMPLATEMATCH_BOX_COLOR, TEMPLATEMATCH_TEXT_COLOR, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path, FACEDETECT_VID_BOOL, TEMPLATEMATCH_VID_BOOL, OPFLOW_VID_BOOL, SIFT_VID_BOOL, OPFLOW_TOTAL_RANDOM_COLORS, OPFLOW_COLOR)\n",
    "    else:\n",
    "        for i in range(len(video_paths_list)):\n",
    "            filename = f'super_{i+1}'\n",
    "            full_output_path = os.path.join(OUTPUT_FOLDER, filename)\n",
    "            super_video(video_paths_list[i], VID_UPDATE_RATE, FACEDETECTION_FRONT, SCALEFACTOR_F, MINNEIGHBORS_F, MINSIZE_F, MAXSIZE_F, FACEDETECTION_SIDE, SCALEFACTOR_S, MINNEIGHBORS_S, MINSIZE_S, MAXSIZE_S, EMOTION_CLASSIFIER, GENDER_CLASSIFIER, EMOTION_OFFSETS, GENDER_OFFSETS, EMOTION_LABELS, GENDER_LABELS, template, TEMPLATEMATCH_METHODS, TM_METHODS_COLOR, TEMPLATEMATCH_VID_METHODCHOICE, sift_image, SIFT_RATIO_THRESHOLD, SIFT_LINE_COLOR, SIFT_POINT_COLOR, SIFT_RSIDE, SIFT_FLAG, BOX_COLOR, BOX_THICK, TEXT_COLOR, TEXT_SIZE, TEXT_THICK, TEXT_COLOR_TF, TEMPLATEMATCH_BOX_COLOR, TEMPLATEMATCH_TEXT_COLOR, OUTLINE_TEXT_BOOL, OUTLINE_TEXT_COLOR, full_output_path, FACEDETECT_VID_BOOL, TEMPLATEMATCH_VID_BOOL, OPFLOW_VID_BOOL, SIFT_VID_BOOL, OPFLOW_TOTAL_RANDOM_COLORS, OPFLOW_COLOR)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
